<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.2.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/favicon.ico?v=6.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon.ico?v=6.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon.ico?v=6.2.0">


  <link rel="mask-icon" href="/assets/images/favicon.ico?v=6.2.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.2.0',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="Machine-Learning,Logistic-Regression," />


<meta name="description" content="1.引言先说一句，年末双十一什么的一来，真是非(mang)常(cheng)欢(gou)乐(le)！然后push自己抽出时间来写这篇blog的原因也非常简单：  写完前两篇逻辑回归的介绍和各个角度理解之后，我们讨论群(戳我入群)的小伙伴们纷纷表示『好像很高级的样纸，but 然并卵 啊！你们倒是拿点实际数据来给我们看看，这玩意儿 有！什！么！用！啊！』 talk is cheap, show me t">
<meta name="keywords" content="Machine-Learning,Logistic-Regression">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习系列(3)_逻辑回归应用之Kaggle泰坦尼克之灾">
<meta property="og:url" content="http://hyperj.github.com/2015/2015-11-12-ml-logistic-regression-3/index.html">
<meta property="og:site_name" content="HyperJ&#39;s Blog!">
<meta property="og:description" content="1.引言先说一句，年末双十一什么的一来，真是非(mang)常(cheng)欢(gou)乐(le)！然后push自己抽出时间来写这篇blog的原因也非常简单：  写完前两篇逻辑回归的介绍和各个角度理解之后，我们讨论群(戳我入群)的小伙伴们纷纷表示『好像很高级的样纸，but 然并卵 啊！你们倒是拿点实际数据来给我们看看，这玩意儿 有！什！么！用！啊！』 talk is cheap, show me t">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/001.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/002.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/003.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/004.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/005.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/006.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/007.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/008.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/009.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/010.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/011.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/012.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/013.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/014.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/015.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/016.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/017.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/018.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/019.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/020.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/021.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/022.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/023.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/024.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/025.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/026.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/027.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/028.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/029.png">
<meta property="og:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/030.png">
<meta property="og:updated_time" content="2018-03-11T10:08:30.910Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习系列(3)_逻辑回归应用之Kaggle泰坦尼克之灾">
<meta name="twitter:description" content="1.引言先说一句，年末双十一什么的一来，真是非(mang)常(cheng)欢(gou)乐(le)！然后push自己抽出时间来写这篇blog的原因也非常简单：  写完前两篇逻辑回归的介绍和各个角度理解之后，我们讨论群(戳我入群)的小伙伴们纷纷表示『好像很高级的样纸，but 然并卵 啊！你们倒是拿点实际数据来给我们看看，这玩意儿 有！什！么！用！啊！』 talk is cheap, show me t">
<meta name="twitter:image" content="http://hyperj.github.com/assets/images/2015/11/12/ml-logistic-regression-3/001.png">



  <link rel="alternate" href="/atom.xml" title="HyperJ's Blog!" type="application/atom+xml" />




  <link rel="canonical" href="http://hyperj.github.com/2015/2015-11-12-ml-logistic-regression-3/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>机器学习系列(3)_逻辑回归应用之Kaggle泰坦尼克之灾 | HyperJ's Blog!</title>
  






  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?35d7ac00b108e4e02793e4abb6d9f575";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">HyperJ's Blog!</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">Data, Distribution, Containerization & Artificial Intelligence</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
        </li>
      
        
        <li class="menu-item menu-item-work">
          <a href="/work/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />Work</a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />Projects</a>
        </li>
      
        
        <li class="menu-item menu-item-collections">
          <a href="/collections/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-list"></i> <br />Collections</a>
        </li>
      
        
        <li class="menu-item menu-item-specials">
          <a href="/specials/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-check-square"></i> <br />Specials</a>
        </li>
      
        
        <li class="menu-item menu-item-translations">
          <a href="/translations/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-exchange"></i> <br />Translations</a>
        </li>
      
        
        <li class="menu-item menu-item-links">
          <a href="/links/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-link"></i> <br />Links</a>
        </li>
      
        
        <li class="menu-item menu-item-books">
          <a href="/books/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-book"></i> <br />Books</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags</a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-user"></i> <br />About</a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />Search</a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://hyperj.github.com/2015/2015-11-12-ml-logistic-regression-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HyperJ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/assets/images/hyperj.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HyperJ's Blog!">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">机器学习系列(3)_逻辑回归应用之Kaggle泰坦尼克之灾</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2015-11-12T00:00:00+08:00">2015-11-12</time>
            

            
            
              
                
              
            

            
              
              <span class="post-meta-divider">|</span>
              

              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2018-03-11T18:08:30+08:00">2018-03-11</time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine-Learning</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon"
            >
            <i class="fa fa-eye"></i>
             Views: 
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>先说一句，年末双十一什么的一来，真是非(mang)常(cheng)欢(gou)乐(le)！然后push自己抽出时间来写这篇blog的原因也非常简单：</p>
<ul>
<li>写完前两篇逻辑回归的介绍和各个角度理解之后，我们讨论群(戳我入群)的小伙伴们纷纷表示『好像很高级的样纸，but 然并卵 啊！你们倒是拿点实际数据来给我们看看，这玩意儿 有！什！么！用！啊！』</li>
<li>talk is cheap, show me the code！</li>
<li>no example say a jb！</li>
</ul>
<p>OK，OK，这就来了咯，同学们别着急，我们先找个简单的实际例子，来看看，所谓的数据挖掘或者机器学习实际应用到底是怎么样一个过程。</p>
<p>『喂，那几个说要看大数据上机器学习应用的，对，就是说你们！别着急好么，我们之后拉点大一点实际数据用liblinear或者spark,MLlib跑给你们看，行不行？咱们先拿个实例入入门嘛』</p>
<p>好了，我是一个严肃的技术研究和分享者，咳咳，不能废话了，各位同学继续往下看吧！</p>
<h2 id="2-背景"><a href="#2-背景" class="headerlink" title="2.背景"></a>2.背景</h2><h3 id="2-1-关于Kaggle"><a href="#2-1-关于Kaggle" class="headerlink" title="2.1 关于Kaggle"></a>2.1 关于Kaggle</h3><ul>
<li>我是Kaggle地址，翻我牌子</li>
<li>亲，逼格这么高的地方，你一定听过对不对？是！这就是那个无数『数据挖掘先驱』们，在回答”枪我有了，哪能找到靶子练练手啊？”时候的答案！</li>
<li>这是一个要数据有数据，要实际应用场景有场景，要一起在数据挖掘领域high得不要不要的小伙伴就有小伙伴的地方啊！！！</li>
</ul>
<p>艾玛，逗逼模式开太猛了。恩，不闹，不闹，说正事，Kaggle是一个数据分析建模的应用竞赛平台，有点类似KDD-CUP（国际知识发现和数据挖掘竞赛），企业或者研究者可以将问题背景、数据、期望指标等发布到Kaggle上，以竞赛的形式向广大的数据科学家征集解决方案。而热爱数(dong)据(shou)挖(zhe)掘(teng)的小伙伴们可以下载/分析数据，使用统计/机器学习/数据挖掘等知识，建立算法模型，得出结果并提交，排名top的可能会有奖金哦！</p>
<h3 id="2-2-关于泰坦尼克号之灾"><a href="#2-2-关于泰坦尼克号之灾" class="headerlink" title="2.2 关于泰坦尼克号之灾"></a>2.2 关于泰坦尼克号之灾</h3><p>带大家去该问题页面溜达一圈吧</p>
<ul>
<li>下面是问题背景页</li>
</ul>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/001.png" alt=""></p>
<ul>
<li>下面是可下载Data的页面 </li>
</ul>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/002.png" alt=""></p>
<ul>
<li>下面是小伙伴们最爱的forum页面，你会看到各种神级人物厉(qi)害(pa)的数据处理/建模想法，你会直视『世界真奇妙』。 </li>
</ul>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/003.png" alt=""></p>
<p>泰坦尼克号问题之背景</p>
<ul>
<li><p>就是那个大家都熟悉的『Jack and Rose』的故事，豪华游艇倒了，大家都惊恐逃生，可是救生艇的数量有限，无法人人都有，副船长发话了『lady and kid first！』，所以是否获救其实并非随机，而是基于一些背景有rank先后的。</p>
</li>
<li><p>训练和测试数据是一些乘客的个人信息以及存活状况，要尝试根据它生成合适的模型并预测其他人的存活状况。</p>
</li>
<li><p>对，这是一个二分类问题，是我们之前讨论的logistic regression所能处理的范畴。</p>
</li>
</ul>
<h2 id="3-说明"><a href="#3-说明" class="headerlink" title="3.说明"></a>3.说明</h2><p>接触过Kaggle的同学们可能知道这个问题，也可能知道RandomForest和SVM等等算法，甚至还对多个模型做过融合，取得过非常好的结果，那maybe这篇文章并不是针对你的，你可以自行略过。</p>
<p>我们因为之前只介绍了Logistic Regression这一种分类算法。所以本次的问题解决过程和优化思路，都集中在这种算法上。其余的方法可能我们之后的文章里会提到。</p>
<p>说点个人的观点。不一定正确。 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">『解决一个问题的方法和思路不止一种』 </span><br><span class="line">『没有所谓的机器学习算法优劣，也没有绝对高性能的机器学习算法，只有在特定的场景、数据和特征下更合适的机器学习算法。』</span><br></pre></td></tr></table></figure>
<h2 id="4-怎么做？"><a href="#4-怎么做？" class="headerlink" title="4.怎么做？"></a>4.怎么做？</h2><p>手把手教程马上就来，先来两条我看到的，觉得很重要的经验。</p>
<ul>
<li><p>印象中Andrew Ng老师似乎在coursera上说过，应用机器学习，千万不要一上来就试图做到完美，先撸一个baseline的model出来，再进行后续的分析步骤，一步步提高，所谓后续步骤可能包括『分析model现在的状态(欠/过拟合)，分析我们使用的feature的作用大小，进行feature selection，以及我们模型下的bad case和产生的原因』等等。</p>
</li>
<li><p>Kaggle上的大神们，也分享过一些experience，说几条我记得的哈：</p>
<ul>
<li>『对数据的认识太重要了！』</li>
<li>『数据中的特殊点/离群点的分析和处理太重要了！』</li>
<li>『特征工程(feature engineering)太重要了！在很多Kaggle的场景下，甚至比model本身还要重要』</li>
<li>『要做模型融合(model ensemble)啊啊啊！』</li>
</ul>
</li>
</ul>
<h2 id="5-初探数据"><a href="#5-初探数据" class="headerlink" title="5.初探数据"></a>5.初探数据</h2><p>先看看我们的数据，长什么样吧。在Data下我们train.csv和test.csv两个文件，分别存着官方给的训练和测试数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment">#数据分析</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#科学计算</span></span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> Series,DataFrame</span><br><span class="line"></span><br><span class="line">data_train = pd.read_csv(<span class="string">"/Users/Hanxiaoyang/Titanic_data/Train.csv"</span>)</span><br><span class="line">data_train</span><br></pre></td></tr></table></figure>
<p>pandas是常用的Python数据处理包，把csv文件读入成dataframe各式，我们在ipython notebook中，看到data_train如下所示：</p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/004.png" alt=""></p>
<p>这就是典型的dataframe格式，如果你没接触过这种格式，完全没有关系，你就把它想象成Excel里面的列好了。 </p>
<p>我们看到，总共有12列，其中Survived字段表示的是该乘客是否获救，其余都是乘客的个人信息，包括：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PassengerId =&gt; 乘客ID</span><br><span class="line">Pclass =&gt; 乘客等级(1/2/3等舱位)</span><br><span class="line">Name =&gt; 乘客姓名</span><br><span class="line">Sex =&gt; 性别</span><br><span class="line">Age =&gt; 年龄</span><br><span class="line">SibSp =&gt; 堂兄弟/妹个数</span><br><span class="line">Parch =&gt; 父母与小孩个数</span><br><span class="line">Ticket =&gt; 船票信息</span><br><span class="line">Fare =&gt; 票价</span><br><span class="line">Cabin =&gt; 客舱</span><br><span class="line">Embarked =&gt; 登船港口</span><br></pre></td></tr></table></figure>
<p>逐条往下看，要看完这么多条，眼睛都有一种要瞎的赶脚。好吧，我们让dataframe自己告诉我们一些信息，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_train.info()</span><br></pre></td></tr></table></figure>
<p>看到了如下的信息： </p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/005.png" alt=""></p>
<p>上面的数据说啥了？它告诉我们，训练数据中总共有891名乘客，但是很不幸，我们有些属性的数据不全，比如说：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Age（年龄）属性只有714名乘客有记录</span><br><span class="line">Cabin（客舱）更是只有204名乘客是已知的</span><br></pre></td></tr></table></figure>
<p>似乎信息略少啊，想再瞄一眼具体数据数值情况呢？恩，我们用下列的方法，得到数值型数据的一些分布(因为有些属性，比如姓名，是文本型；而另外一些属性，比如登船港口，是类目型。这些我们用下面的函数是看不到的)：</p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/006.png" alt=""></p>
<p>我们从上面看到更进一步的什么信息呢？ </p>
<p>mean字段告诉我们，大概0.383838的人最后获救了，2/3等舱的人数比1等舱要多，平均乘客年龄大概是29.7岁(计算这个时候会略掉无记录的)等等…</p>
<h2 id="6-数据初步分析"><a href="#6-数据初步分析" class="headerlink" title="6.数据初步分析"></a>6.数据初步分析</h2><p>每个乘客都这么多属性，那我们咋知道哪些属性更有用，而又应该怎么用它们啊？说实话这会儿我也不知道，但我们记得前面提到过</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">『对数据的认识太重要了！』</span><br><span class="line">『对数据的认识太重要了！』</span><br><span class="line">『对数据的认识太重要了！』</span><br></pre></td></tr></table></figure>
<p>重要的事情说三遍，恩，说完了。仅仅最上面的对数据了解，依旧无法给我们提供想法和思路。我们再深入一点来看看我们的数据，看看每个/多个 属性和最后的Survived之间有着什么样的关系呢。</p>
<h3 id="6-1-乘客各属性分布"><a href="#6-1-乘客各属性分布" class="headerlink" title="6.1 乘客各属性分布"></a>6.1 乘客各属性分布</h3><p>脑容量太有限了…数值看花眼了。我们还是统计统计，画些图来看看属性和结果之间的关系好了，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig = plt.figure()</span><br><span class="line">fig.set(alpha=<span class="number">0.2</span>)  <span class="comment"># 设定图表颜色alpha参数</span></span><br><span class="line"></span><br><span class="line">plt.subplot2grid((<span class="number">2</span>,<span class="number">3</span>),(<span class="number">0</span>,<span class="number">0</span>))             <span class="comment"># 在一张大图里分列几个小图</span></span><br><span class="line">data_train.Survived.value_counts().plot(kind=<span class="string">'bar'</span>)<span class="comment"># 柱状图 </span></span><br><span class="line">plt.title(<span class="string">u"获救情况 (1为获救)"</span>) <span class="comment"># 标题</span></span><br><span class="line">plt.ylabel(<span class="string">u"人数"</span>)  </span><br><span class="line"></span><br><span class="line">plt.subplot2grid((<span class="number">2</span>,<span class="number">3</span>),(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">data_train.Pclass.value_counts().plot(kind=<span class="string">"bar"</span>)</span><br><span class="line">plt.ylabel(<span class="string">u"人数"</span>)</span><br><span class="line">plt.title(<span class="string">u"乘客等级分布"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot2grid((<span class="number">2</span>,<span class="number">3</span>),(<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line">plt.scatter(data_train.Survived, data_train.Age)</span><br><span class="line">plt.ylabel(<span class="string">u"年龄"</span>)                         <span class="comment"># 设定纵坐标名称</span></span><br><span class="line">plt.grid(b=<span class="keyword">True</span>, which=<span class="string">'major'</span>, axis=<span class="string">'y'</span>) </span><br><span class="line">plt.title(<span class="string">u"按年龄看获救分布 (1为获救)"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot2grid((<span class="number">2</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">0</span>), colspan=<span class="number">2</span>)</span><br><span class="line">data_train.Age[data_train.Pclass == <span class="number">1</span>].plot(kind=<span class="string">'kde'</span>)   </span><br><span class="line">data_train.Age[data_train.Pclass == <span class="number">2</span>].plot(kind=<span class="string">'kde'</span>)</span><br><span class="line">data_train.Age[data_train.Pclass == <span class="number">3</span>].plot(kind=<span class="string">'kde'</span>)</span><br><span class="line">plt.xlabel(<span class="string">u"年龄"</span>)<span class="comment"># plots an axis lable</span></span><br><span class="line">plt.ylabel(<span class="string">u"密度"</span>) </span><br><span class="line">plt.title(<span class="string">u"各等级的乘客年龄分布"</span>)</span><br><span class="line">plt.legend((<span class="string">u'头等舱'</span>, <span class="string">u'2等舱'</span>,<span class="string">u'3等舱'</span>),loc=<span class="string">'best'</span>) <span class="comment"># sets our legend for our graph.</span></span><br><span class="line"></span><br><span class="line">plt.subplot2grid((<span class="number">2</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">data_train.Embarked.value_counts().plot(kind=<span class="string">'bar'</span>)</span><br><span class="line">plt.title(<span class="string">u"各登船口岸上船人数"</span>)</span><br><span class="line">plt.ylabel(<span class="string">u"人数"</span>)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/007.png" alt=""></p>
<p>bingo，图还是比数字好看多了。所以我们在图上可以看出来，被救的人300多点，不到半数；3等舱乘客灰常多；遇难和获救的人年龄似乎跨度都很广；3个不同的舱年龄总体趋势似乎也一致，2/3等舱乘客20岁多点的人最多，1等舱40岁左右的最多(→_→似乎符合财富和年龄的分配哈，咳咳，别理我，我瞎扯的)；登船港口人数按照S、C、Q递减，而且S远多于另外俩港口。</p>
<p>这个时候我们可能会有一些想法了：</p>
<ul>
<li>不同舱位/乘客等级可能和财富/地位有关系，最后获救概率可能会不一样</li>
<li>年龄对获救概率也一定是有影响的，毕竟前面说了，副船长还说『小孩和女士先走』呢</li>
<li>和登船港口是不是有关系呢？也许登船港口不同，人的出身地位不同？</li>
</ul>
<p>口说无凭，空想无益。老老实实再来统计统计，看看这些属性值的统计分布吧。</p>
<h3 id="6-2-属性与获救结果的关联统计"><a href="#6-2-属性与获救结果的关联统计" class="headerlink" title="6.2 属性与获救结果的关联统计"></a>6.2 属性与获救结果的关联统计</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#看看各乘客等级的获救情况</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">fig.set(alpha=<span class="number">0.2</span>)  <span class="comment"># 设定图表颜色alpha参数</span></span><br><span class="line"></span><br><span class="line">Survived_0 = data_train.Pclass[data_train.Survived == <span class="number">0</span>].value_counts()</span><br><span class="line">Survived_1 = data_train.Pclass[data_train.Survived == <span class="number">1</span>].value_counts()</span><br><span class="line">df=pd.DataFrame(&#123;<span class="string">u'获救'</span>:Survived_1, <span class="string">u'未获救'</span>:Survived_0&#125;)</span><br><span class="line">df.plot(kind=<span class="string">'bar'</span>, stacked=<span class="keyword">True</span>)</span><br><span class="line">plt.title(<span class="string">u"各乘客等级的获救情况"</span>)</span><br><span class="line">plt.xlabel(<span class="string">u"乘客等级"</span>) </span><br><span class="line">plt.ylabel(<span class="string">u"人数"</span>) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/008.png" alt=""></p>
<p>啧啧，果然，钱和地位对舱位有影响，进而对获救的可能性也有影响啊←_← </p>
<p>咳咳，跑题了，我想说的是，明显等级为1的乘客，获救的概率高很多。恩，这个一定是影响最后获救结果的一个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#看看各性别的获救情况</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">fig.set(alpha=<span class="number">0.2</span>)  <span class="comment"># 设定图表颜色alpha参数</span></span><br><span class="line"></span><br><span class="line">Survived_m = data_train.Survived[data_train.Sex == <span class="string">'male'</span>].value_counts()</span><br><span class="line">Survived_f = data_train.Survived[data_train.Sex == <span class="string">'female'</span>].value_counts()</span><br><span class="line">df=pd.DataFrame(&#123;<span class="string">u'男性'</span>:Survived_m, <span class="string">u'女性'</span>:Survived_f&#125;)</span><br><span class="line">df.plot(kind=<span class="string">'bar'</span>, stacked=<span class="keyword">True</span>)</span><br><span class="line">plt.title(<span class="string">u"按性别看获救情况"</span>)</span><br><span class="line">plt.xlabel(<span class="string">u"性别"</span>) </span><br><span class="line">plt.ylabel(<span class="string">u"人数"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/009.png" alt=""></p>
<p>歪果盆友果然很尊重lady，lady first践行得不错。性别无疑也要作为重要特征加入最后的模型之中。</p>
<p>再来个详细版的好了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#然后我们再来看看各种舱级别情况下各性别的获救情况</span></span><br><span class="line">fig=plt.figure()</span><br><span class="line">fig.set(alpha=<span class="number">0.65</span>) <span class="comment"># 设置图像透明度，无所谓</span></span><br><span class="line">plt.title(<span class="string">u"根据舱等级和性别的获救情况"</span>)</span><br><span class="line"></span><br><span class="line">ax1=fig.add_subplot(<span class="number">141</span>)</span><br><span class="line">data_train.Survived[data_train.Sex == <span class="string">'female'</span>][data_train.Pclass != <span class="number">3</span>].value_counts().plot(kind=<span class="string">'bar'</span>, label=<span class="string">"female highclass"</span>, color=<span class="string">'#FA2479'</span>)</span><br><span class="line">ax1.set_xticklabels([<span class="string">u"获救"</span>, <span class="string">u"未获救"</span>], rotation=<span class="number">0</span>)</span><br><span class="line">ax1.legend([<span class="string">u"女性/高级舱"</span>], loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">ax2=fig.add_subplot(<span class="number">142</span>, sharey=ax1)</span><br><span class="line">data_train.Survived[data_train.Sex == <span class="string">'female'</span>][data_train.Pclass == <span class="number">3</span>].value_counts().plot(kind=<span class="string">'bar'</span>, label=<span class="string">'female, low class'</span>, color=<span class="string">'pink'</span>)</span><br><span class="line">ax2.set_xticklabels([<span class="string">u"未获救"</span>, <span class="string">u"获救"</span>], rotation=<span class="number">0</span>)</span><br><span class="line">plt.legend([<span class="string">u"女性/低级舱"</span>], loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">ax3=fig.add_subplot(<span class="number">143</span>, sharey=ax1)</span><br><span class="line">data_train.Survived[data_train.Sex == <span class="string">'male'</span>][data_train.Pclass != <span class="number">3</span>].value_counts().plot(kind=<span class="string">'bar'</span>, label=<span class="string">'male, high class'</span>,color=<span class="string">'lightblue'</span>)</span><br><span class="line">ax3.set_xticklabels([<span class="string">u"未获救"</span>, <span class="string">u"获救"</span>], rotation=<span class="number">0</span>)</span><br><span class="line">plt.legend([<span class="string">u"男性/高级舱"</span>], loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">ax4=fig.add_subplot(<span class="number">144</span>, sharey=ax1)</span><br><span class="line">data_train.Survived[data_train.Sex == <span class="string">'male'</span>][data_train.Pclass == <span class="number">3</span>].value_counts().plot(kind=<span class="string">'bar'</span>, label=<span class="string">'male low class'</span>, color=<span class="string">'steelblue'</span>)</span><br><span class="line">ax4.set_xticklabels([<span class="string">u"未获救"</span>, <span class="string">u"获救"</span>], rotation=<span class="number">0</span>)</span><br><span class="line">plt.legend([<span class="string">u"男性/低级舱"</span>], loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/010.png" alt=""></p>
<p>恩，坚定了之前的判断。</p>
<p>我们看看各登船港口的获救情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">fig.set(alpha=<span class="number">0.2</span>)  <span class="comment"># 设定图表颜色alpha参数</span></span><br><span class="line"></span><br><span class="line">Survived_0 = data_train.Embarked[data_train.Survived == <span class="number">0</span>].value_counts()</span><br><span class="line">Survived_1 = data_train.Embarked[data_train.Survived == <span class="number">1</span>].value_counts()</span><br><span class="line">df=pd.DataFrame(&#123;<span class="string">u'获救'</span>:Survived_1, <span class="string">u'未获救'</span>:Survived_0&#125;)</span><br><span class="line">df.plot(kind=<span class="string">'bar'</span>, stacked=<span class="keyword">True</span>)</span><br><span class="line">plt.title(<span class="string">u"各登录港口乘客的获救情况"</span>)</span><br><span class="line">plt.xlabel(<span class="string">u"登录港口"</span>) </span><br><span class="line">plt.ylabel(<span class="string">u"人数"</span>) </span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/011.png" alt=""></p>
<p>下面我们来看看 堂兄弟/妹，孩子/父母有几人，对是否获救的影响。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">g = data_train.groupby([<span class="string">'SibSp'</span>,<span class="string">'Survived'</span>])</span><br><span class="line">df = pd.DataFrame(g.count()[<span class="string">'PassengerId'</span>])</span><br><span class="line"><span class="keyword">print</span> df</span><br><span class="line"></span><br><span class="line">g = data_train.groupby([<span class="string">'SibSp'</span>,<span class="string">'Survived'</span>])</span><br><span class="line">df = pd.DataFrame(g.count()[<span class="string">'PassengerId'</span>])</span><br><span class="line"><span class="keyword">print</span> df</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/012.png" alt=""></p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/013.png" alt=""></p>
<p>好吧，没看出特别特别明显的规律(为自己的智商感到捉急…)，先作为备选特征，放一放。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#ticket是船票编号，应该是unique的，和最后的结果没有太大的关系，先不纳入考虑的特征范畴把</span></span><br><span class="line"><span class="comment">#cabin只有204个乘客有值，我们先看看它的一个分布</span></span><br><span class="line">data_train.Cabin.value_counts()</span><br></pre></td></tr></table></figure>
<p>部分结果如下： </p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/014.png" alt=""></p>
<p>这三三两两的…如此不集中…我们猜一下，也许，前面的ABCDE是指的甲板位置、然后编号是房间号？…好吧，我瞎说的，别当真…</p>
<p>关键是Cabin这鬼属性，应该算作类目型的，本来缺失值就多，还如此不集中，注定是个棘手货…第一感觉，这玩意儿如果直接按照类目特征处理的话，太散了，估计每个因子化后的特征都拿不到什么权重。加上有那么多缺失值，要不我们先把Cabin缺失与否作为条件(虽然这部分信息缺失可能并非未登记，maybe只是丢失了而已，所以这样做未必妥当)，先在有无Cabin信息这个粗粒度上看看Survived的情况好了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">fig.set(alpha=<span class="number">0.2</span>)  <span class="comment"># 设定图表颜色alpha参数</span></span><br><span class="line"></span><br><span class="line">Survived_cabin = data_train.Survived[pd.notnull(data_train.Cabin)].value_counts()</span><br><span class="line">Survived_nocabin = data_train.Survived[pd.isnull(data_train.Cabin)].value_counts()</span><br><span class="line">df=pd.DataFrame(&#123;<span class="string">u'有'</span>:Survived_cabin, <span class="string">u'无'</span>:Survived_nocabin&#125;).transpose()</span><br><span class="line">df.plot(kind=<span class="string">'bar'</span>, stacked=<span class="keyword">True</span>)</span><br><span class="line">plt.title(<span class="string">u"按Cabin有无看获救情况"</span>)</span><br><span class="line">plt.xlabel(<span class="string">u"Cabin有无"</span>) </span><br><span class="line">plt.ylabel(<span class="string">u"人数"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/015.png" alt=""></p>
<p>咳咳，有Cabin记录的似乎获救概率稍高一些，先这么着放一放吧。</p>
<h2 id="7-简单数据预处理"><a href="#7-简单数据预处理" class="headerlink" title="7.简单数据预处理"></a>7.简单数据预处理</h2><p>大体数据的情况看了一遍，对感兴趣的属性也有个大概的了解了。 </p>
<p>下一步干啥？咱们该处理处理这些数据，为机器学习建模做点准备了。</p>
<p>对了，我这里说的数据预处理，其实就包括了很多Kaggler津津乐道的feature engineering过程，灰常灰常有必要！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">『特征工程(feature engineering)太重要了！』 </span><br><span class="line">『特征工程(feature engineering)太重要了！』 </span><br><span class="line">『特征工程(feature engineering)太重要了！』</span><br></pre></td></tr></table></figure>
<p>恩，重要的事情说三遍。</p>
<p>先从最突出的数据属性开始吧，对，Cabin和Age，有丢失数据实在是对下一步工作影响太大。</p>
<p>先说Cabin，暂时我们就按照刚才说的，按Cabin有无数据，将这个属性处理成Yes和No两种类型吧。</p>
<p>再说Age：</p>
<p>通常遇到缺值的情况，我们会有几种常见的处理方式</p>
<ul>
<li>如果缺值的样本占总数比例极高，我们可能就直接舍弃了，作为特征加入的话，可能反倒带入noise，影响最后的结果了</li>
<li>如果缺值的样本适中，而该属性非连续值特征属性(比如说类目属性)，那就把NaN作为一个新类别，加到类别特征中</li>
<li>如果缺值的样本适中，而该属性为连续值特征属性，有时候我们会考虑给定一个step(比如这里的age，我们可以考虑每隔2/3岁为一个步长)，然后把它离散化，之后把NaN作为一个type加到属性类目中。</li>
<li>有些情况下，缺失的值个数并不是特别多，那我们也可以试着根据已有的值，拟合一下数据，补充上。</li>
</ul>
<p>本例中，后两种处理方式应该都是可行的，我们先试试拟合补全吧(虽然说没有特别多的背景可供我们拟合，这不一定是一个多么好的选择)</p>
<p>我们这里用scikit-learn中的RandomForest来拟合一下缺失的年龄数据(注：RandomForest是一个用在原始数据中做不同采样，建立多颗DecisionTree，再进行average等等来降低过拟合现象，提高结果的机器学习算法，我们之后会介绍到)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment">### 使用 RandomForestClassifier 填补缺失的年龄属性</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_missing_ages</span><span class="params">(df)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把已有的数值型特征取出来丢进Random Forest Regressor中</span></span><br><span class="line">    age_df = df[[<span class="string">'Age'</span>,<span class="string">'Fare'</span>, <span class="string">'Parch'</span>, <span class="string">'SibSp'</span>, <span class="string">'Pclass'</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 乘客分成已知年龄和未知年龄两部分</span></span><br><span class="line">    known_age = age_df[age_df.Age.notnull()].as_matrix()</span><br><span class="line">    unknown_age = age_df[age_df.Age.isnull()].as_matrix()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># y即目标年龄</span></span><br><span class="line">    y = known_age[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># X即特征属性值</span></span><br><span class="line">    X = known_age[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># fit到RandomForestRegressor之中</span></span><br><span class="line">    rfr = RandomForestRegressor(random_state=<span class="number">0</span>, n_estimators=<span class="number">2000</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">    rfr.fit(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用得到的模型进行未知年龄结果预测</span></span><br><span class="line">    predictedAges = rfr.predict(unknown_age[:, <span class="number">1</span>::])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用得到的预测结果填补原缺失数据</span></span><br><span class="line">    df.loc[ (df.Age.isnull()), <span class="string">'Age'</span> ] = predictedAges </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> df, rfr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_Cabin_type</span><span class="params">(df)</span>:</span></span><br><span class="line">    df.loc[ (df.Cabin.notnull()), <span class="string">'Cabin'</span> ] = <span class="string">"Yes"</span></span><br><span class="line">    df.loc[ (df.Cabin.isnull()), <span class="string">'Cabin'</span> ] = <span class="string">"No"</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line">data_train, rfr = set_missing_ages(data_train)</span><br><span class="line">data_train = set_Cabin_type(data_train)</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/016.png" alt=""></p>
<p>恩。目的达到，OK了。</p>
<p>因为逻辑回归建模时，需要输入的特征都是数值型特征，我们通常会先对类目型的特征因子化。 </p>
<p>什么叫做因子化呢？举个例子：</p>
<p>以Cabin为例，原本一个属性维度，因为其取值可以是[‘yes’,’no’]，而将其平展开为’Cabin_yes’,’Cabin_no’两个属性</p>
<ul>
<li>原本Cabin取值为yes的，在此处的”Cabin_yes”下取值为1，在”Cabin_no”下取值为0</li>
<li>原本Cabin取值为no的，在此处的”Cabin_yes”下取值为0，在”Cabin_no”下取值为1</li>
</ul>
<p>我们使用pandas的”get_dummies”来完成这个工作，并拼接在原来的”data_train”之上，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dummies_Cabin = pd.get_dummies(data_train[<span class="string">'Cabin'</span>], prefix= <span class="string">'Cabin'</span>)</span><br><span class="line"></span><br><span class="line">dummies_Embarked = pd.get_dummies(data_train[<span class="string">'Embarked'</span>], prefix= <span class="string">'Embarked'</span>)</span><br><span class="line"></span><br><span class="line">dummies_Sex = pd.get_dummies(data_train[<span class="string">'Sex'</span>], prefix= <span class="string">'Sex'</span>)</span><br><span class="line"></span><br><span class="line">dummies_Pclass = pd.get_dummies(data_train[<span class="string">'Pclass'</span>], prefix= <span class="string">'Pclass'</span>)</span><br><span class="line"></span><br><span class="line">df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=<span class="number">1</span>)</span><br><span class="line">df.drop([<span class="string">'Pclass'</span>, <span class="string">'Name'</span>, <span class="string">'Sex'</span>, <span class="string">'Ticket'</span>, <span class="string">'Cabin'</span>, <span class="string">'Embarked'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/017.png" alt=""></p>
<p>bingo，我们很成功地把这些类目属性全都转成0，1的数值属性了。</p>
<p>这样，看起来，是不是我们需要的属性值都有了，且它们都是数值型属性呢。</p>
<p>有一种临近结果的宠宠欲动感吧，莫急莫急，我们还得做一些处理，仔细看看Age和Fare两个属性，乘客的数值幅度变化，也忒大了吧！！如果大家了解逻辑回归与梯度下降的话，会知道，各属性值之间scale差距太大，将对收敛速度造成几万点伤害值！甚至不收敛！ (╬▔皿▔)…所以我们先用scikit-learn里面的preprocessing模块对这俩货做一个scaling，所谓scaling，其实就是将一些变化幅度较大的特征化到[-1,1]之内。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> preprocessing</span><br><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">age_scale_param = scaler.fit(df[<span class="string">'Age'</span>])</span><br><span class="line">df[<span class="string">'Age_scaled'</span>] = scaler.fit_transform(df[<span class="string">'Age'</span>], age_scale_param)</span><br><span class="line">fare_scale_param = scaler.fit(df[<span class="string">'Fare'</span>])</span><br><span class="line">df[<span class="string">'Fare_scaled'</span>] = scaler.fit_transform(df[<span class="string">'Fare'</span>], fare_scale_param)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/018.png" alt=""></p>
<p>恩，好看多了，万事俱备，只欠建模。马上就要看到成效了，哈哈。我们把需要的属性值抽出来，转成scikit-learn里面LogisticRegression可以处理的格式。</p>
<h2 id="8-逻辑回归建模"><a href="#8-逻辑回归建模" class="headerlink" title="8.逻辑回归建模"></a>8.逻辑回归建模</h2><p>我们把需要的feature字段取出来，转成numpy格式，使用scikit-learn中的LogisticRegression建模。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用正则取出我们要的属性值</span></span><br><span class="line">train_df = df.filter(regex=<span class="string">'Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*'</span>)</span><br><span class="line">train_np = train_df.as_matrix()</span><br><span class="line"></span><br><span class="line"><span class="comment"># y即Survival结果</span></span><br><span class="line">y = train_np[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># X即特征属性值</span></span><br><span class="line">X = train_np[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit到RandomForestRegressor之中</span></span><br><span class="line">clf = linear_model.LogisticRegression(C=<span class="number">1.0</span>, penalty=<span class="string">'l1'</span>, tol=<span class="number">1e-6</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">clf</span><br></pre></td></tr></table></figure>
<p>good，很顺利，我们得到了一个model，如下： </p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/019.png" alt=""></p>
<p>先淡定！淡定！你以为把test.csv直接丢进model里就能拿到结果啊…骚年，图样图森破啊！我们的”test_data”也要做和”train_data”一样的预处理啊！！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">data_test = pd.read_csv(<span class="string">"/Users/Hanxiaoyang/Titanic_data/test.csv"</span>)</span><br><span class="line">data_test.loc[ (data_test.Fare.isnull()), <span class="string">'Fare'</span> ] = <span class="number">0</span></span><br><span class="line"><span class="comment"># 接着我们对test_data做和train_data中一致的特征变换</span></span><br><span class="line"><span class="comment"># 首先用同样的RandomForestRegressor模型填上丢失的年龄</span></span><br><span class="line">tmp_df = data_test[[<span class="string">'Age'</span>,<span class="string">'Fare'</span>, <span class="string">'Parch'</span>, <span class="string">'SibSp'</span>, <span class="string">'Pclass'</span>]]</span><br><span class="line">null_age = tmp_df[data_test.Age.isnull()].as_matrix()</span><br><span class="line"><span class="comment"># 根据特征属性X预测年龄并补上</span></span><br><span class="line">X = null_age[:, <span class="number">1</span>:]</span><br><span class="line">predictedAges = rfr.predict(X)</span><br><span class="line">data_test.loc[ (data_test.Age.isnull()), <span class="string">'Age'</span> ] = predictedAges</span><br><span class="line"></span><br><span class="line">data_test = set_Cabin_type(data_test)</span><br><span class="line">dummies_Cabin = pd.get_dummies(data_test[<span class="string">'Cabin'</span>], prefix= <span class="string">'Cabin'</span>)</span><br><span class="line">dummies_Embarked = pd.get_dummies(data_test[<span class="string">'Embarked'</span>], prefix= <span class="string">'Embarked'</span>)</span><br><span class="line">dummies_Sex = pd.get_dummies(data_test[<span class="string">'Sex'</span>], prefix= <span class="string">'Sex'</span>)</span><br><span class="line">dummies_Pclass = pd.get_dummies(data_test[<span class="string">'Pclass'</span>], prefix= <span class="string">'Pclass'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df_test = pd.concat([data_test, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=<span class="number">1</span>)</span><br><span class="line">df_test.drop([<span class="string">'Pclass'</span>, <span class="string">'Name'</span>, <span class="string">'Sex'</span>, <span class="string">'Ticket'</span>, <span class="string">'Cabin'</span>, <span class="string">'Embarked'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">df_test[<span class="string">'Age_scaled'</span>] = scaler.fit_transform(df_test[<span class="string">'Age'</span>], age_scale_param)</span><br><span class="line">df_test[<span class="string">'Fare_scaled'</span>] = scaler.fit_transform(df_test[<span class="string">'Fare'</span>], fare_scale_param)</span><br><span class="line">df_test</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/020.png" alt=""></p>
<p>不错不错，数据很OK，差最后一步了。 </p>
<p>下面就做预测取结果吧！！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test = df_test.filter(regex=<span class="string">'Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*'</span>)</span><br><span class="line">predictions = clf.predict(test)</span><br><span class="line">result = pd.DataFrame(&#123;<span class="string">'PassengerId'</span>:data_test[<span class="string">'PassengerId'</span>].as_matrix(), <span class="string">'Survived'</span>:predictions.astype(np.int32)&#125;)</span><br><span class="line">result.to_csv(<span class="string">"/Users/Hanxiaoyang/Titanic_data/logistic_regression_predictions.csv"</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/021.png" alt=""></p>
<p>啧啧，挺好，格式正确，去make a submission啦啦啦！</p>
<p>在Kaggle的Make a submission页面，提交上结果。如下： </p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/022.png" alt=""></p>
<p>0.76555，恩，结果还不错。毕竟，这只是我们简单分析处理过后出的一个baseline模型嘛。</p>
<h2 id="9-逻辑回归系统优化"><a href="#9-逻辑回归系统优化" class="headerlink" title="9.逻辑回归系统优化"></a>9.逻辑回归系统优化</h2><h3 id="9-1-模型系数关联分析"><a href="#9-1-模型系数关联分析" class="headerlink" title="9.1 模型系数关联分析"></a>9.1 模型系数关联分析</h3><p>亲，你以为结果提交上了，就完事了？ </p>
<p>我不会告诉你，这只是万里长征第一步啊(泪牛满面)！！！这才刚撸完baseline model啊！！！还得优化啊！！！</p>
<p>看过Andrew Ng老师的machine Learning课程的同学们，知道，我们应该分析分析模型现在的状态了，是过/欠拟合？，以确定我们需要更多的特征还是更多数据，或者其他操作。我们有一条很著名的learning curves对吧。</p>
<p>不过在现在的场景下，先不着急做这个事情，我们这个baseline系统还有些粗糙，先再挖掘挖掘。</p>
<ul>
<li><p>首先，Name和Ticket两个属性被我们完整舍弃了(好吧，其实是因为这俩属性，几乎每一条记录都是一个完全不同的值，我们并没有找到很直接的处理方式)。</p>
</li>
<li><p>然后，我们想想，年龄的拟合本身也未必是一件非常靠谱的事情，我们依据其余属性，其实并不能很好地拟合预测出未知的年龄。再一个，以我们的日常经验，小盆友和老人可能得到的照顾会多一些，这样看的话，年龄作为一个连续值，给一个固定的系数，应该和年龄是一个正相关或者负相关，似乎体现不出两头受照顾的实际情况，所以，说不定我们把年龄离散化，按区段分作类别属性会更合适一些。</p>
</li>
</ul>
<p>上面只是我瞎想的，who knows是不是这么回事呢，老老实实先把得到的model系数和feature关联起来看看。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(&#123;<span class="string">"columns"</span>:list(train_df.columns)[<span class="number">1</span>:], <span class="string">"coef"</span>:list(clf.coef_.T)&#125;)</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/023.png" alt=""></p>
<p>首先，大家回去前两篇文章里瞄一眼公式就知道，这些系数为正的特征，和最后结果是一个正相关，反之为负相关。</p>
<p>我们先看看那些权重绝对值非常大的feature，在我们的模型上：</p>
<ul>
<li>Sex属性，如果是female会极大提高最后获救的概率，而male会很大程度拉低这个概率。</li>
<li>Pclass属性，1等舱乘客最后获救的概率会上升，而乘客等级为3会极大地拉低这个概率。</li>
<li>有Cabin值会很大程度拉升最后获救概率(这里似乎能看到了一点端倪，事实上从最上面的有无Cabin记录的Survived分布图上看出，即使有Cabin记录的乘客也有一部分遇难了，估计这个属性上我们挖掘还不够)</li>
<li>Age是一个负相关，意味着在我们的模型里，年龄越小，越有获救的优先权(还得回原数据看看这个是否合理）</li>
<li>有一个登船港口S会很大程度拉低获救的概率，另外俩港口压根就没啥作用(这个实际上非常奇怪，因为我们从之前的统计图上并没有看到S港口的获救率非常低，所以也许可以考虑把登船港口这个feature去掉试试)。</li>
<li>船票Fare有小幅度的正相关(并不意味着这个feature作用不大，有可能是我们细化的程度还不够，举个例子，说不定我们得对它离散化，再分至各个乘客等级上？)</li>
</ul>
<p>噢啦，观察完了，我们现在有一些想法了，但是怎么样才知道，哪些优化的方法是promising的呢？</p>
<p>因为test.csv里面并没有Survived这个字段(好吧，这是废话，这明明就是我们要预测的结果)，我们无法在这份数据上评定我们算法在该场景下的效果…</p>
<p>而『每做一次调整就make a submission，然后根据结果来判定这次调整的好坏』其实是行不通的…</p>
<h3 id="9-2-交叉验证"><a href="#9-2-交叉验证" class="headerlink" title="9.2 交叉验证"></a>9.2 交叉验证</h3><p>重点又来了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">『要做交叉验证(cross validation)!』 </span><br><span class="line">『要做交叉验证(cross validation)!』 </span><br><span class="line">『要做交叉验证(cross validation)!』</span><br></pre></td></tr></table></figure>
<p>恩，重要的事情说三遍。我们通常情况下，这么做cross validation：把train.csv分成两部分，一部分用于训练我们需要的模型，另外一部分数据上看我们预测算法的效果。</p>
<p>我们用scikit-learn的cross_validation来帮我们完成小数据集上的这个工作。</p>
<p>先简单看看cross validation情况下的打分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> cross_validation</span><br><span class="line"></span><br><span class="line"> <span class="comment">#简单看看打分情况</span></span><br><span class="line">clf = linear_model.LogisticRegression(C=<span class="number">1.0</span>, penalty=<span class="string">'l1'</span>, tol=<span class="number">1e-6</span>)</span><br><span class="line">all_data = df.filter(regex=<span class="string">'Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*'</span>)</span><br><span class="line">X = all_data.as_matrix()[:,<span class="number">1</span>:]</span><br><span class="line">y = all_data.as_matrix()[:,<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> cross_validation.cross_val_score(clf, X, y, cv=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>结果是下面酱紫的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0.81564246 0.81005587 0.78651685 0.78651685 0.81355932]</span><br></pre></td></tr></table></figure>
<p>似乎比Kaggle上的结果略高哈，毕竟用的是不是同一份数据集评估的。</p>
<p>等等，既然我们要做交叉验证，那我们干脆先把交叉验证里面的bad case拿出来看看，看看人眼审核，是否能发现什么蛛丝马迹，是我们忽略了哪些信息，使得这些乘客被判定错了。再把bad case上得到的想法和前头系数分析的合在一起，然后逐个试试。</p>
<p>下面我们做数据分割，并且在原始数据集上瞄一眼bad case：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分割数据，按照 训练数据:cv数据 = 7:3的比例</span></span><br><span class="line">split_train, split_cv = cross_validation.train_test_split(df, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line">train_df = split_train.filter(regex=<span class="string">'Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*'</span>)</span><br><span class="line"><span class="comment"># 生成模型</span></span><br><span class="line">clf = linear_model.LogisticRegression(C=<span class="number">1.0</span>, penalty=<span class="string">'l1'</span>, tol=<span class="number">1e-6</span>)</span><br><span class="line">clf.fit(train_df.as_matrix()[:,<span class="number">1</span>:], train_df.as_matrix()[:,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对cross validation数据进行预测</span></span><br><span class="line"></span><br><span class="line">cv_df = split_cv.filter(regex=<span class="string">'Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*'</span>)</span><br><span class="line">predictions = clf.predict(cv_df.as_matrix()[:,<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line">origin_data_train = pd.read_csv(<span class="string">"/Users/HanXiaoyang/Titanic_data/Train.csv"</span>)</span><br><span class="line">bad_cases = origin_data_train.loc[origin_data_train[<span class="string">'PassengerId'</span>].isin(split_cv[predictions != cv_df.as_matrix()[:,<span class="number">0</span>]][<span class="string">'PassengerId'</span>].values)]</span><br><span class="line">bad_cases</span><br></pre></td></tr></table></figure>
<p>我们判定错误的 bad case 中部分数据如下： </p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/024.png" alt=""></p>
<p>大家可以自己跑一遍试试，拿到bad cases之后，仔细看看。也会有一些猜测和想法。其中会有一部分可能会印证在系数分析部分的猜测，那这些优化的想法优先级可以放高一些。</p>
<p>现在有了”train_df” 和 “vc_df” 两个数据部分，前者用于训练model，后者用于评定和选择模型。可以开始可劲折腾了。</p>
<p>我们随便列一些可能可以做的优化操作：</p>
<ul>
<li>Age属性不使用现在的拟合方式，而是根据名称中的『Mr』『Mrs』『Miss』等的平均值进行填充。</li>
<li>Age不做成一个连续值属性，而是使用一个步长进行离散化，变成离散的类目feature。<br>Cabin再细化一些，对于有记录的Cabin属性，我们将其分为前面的字母部分(我猜是位置和船层之类的信息) 和 后面的数字部分(应该是房间号，有意思的事情是，如果你仔细看看原始数据，你会发现，这个值大的情况下，似乎获救的可能性高一些)。</li>
<li>Pclass和Sex俩太重要了，我们试着用它们去组出一个组合属性来试试，这也是另外一种程度的细化。</li>
<li>单加一个Child字段，Age&lt;=12的，设为1，其余为0(你去看看数据，确实小盆友优先程度很高啊)</li>
<li>如果名字里面有『Mrs』，而Parch&gt;1的，我们猜测她可能是一个母亲，应该获救的概率也会提高，因此可以多加一个Mother字段，此种情况下设为1，其余情况下设为0</li>
<li>登船港口可以考虑先去掉试试(Q和C本来就没权重，S有点诡异)</li>
<li>把堂兄弟/兄妹 和 Parch 还有自己 个数加在一起组一个Family_size字段(考虑到大家族可能对最后的结果有影响)</li>
<li>Name是一个我们一直没有触碰的属性，我们可以做一些简单的处理，比如说男性中带某些字眼的(‘Capt’, ‘Don’, ‘Major’, ‘Sir’)可以统一到一个Title，女性也一样。</li>
</ul>
<p>大家接着往下挖掘，可能还可以想到更多可以细挖的部分。我这里先列这些了，然后我们可以使用手头上的”train_df”和”cv_df”开始试验这些feature engineering的tricks是否有效了。</p>
<p>试验的过程比较漫长，也需要有耐心，而且我们经常会面临很尴尬的状况，就是我们灵光一闪，想到一个feature，然后坚信它一定有效，结果试验下来，效果还不如试验之前的结果。恩，需要坚持和耐心，以及不断的挖掘。</p>
<p>我最好的结果是在『Survived~C(Pclass)+C(Title)+C(Sex)+C(Age_bucket)+C(Cabin_num_bucket)Mother+Fare+Family_Size』下取得的，结果如下(抱歉，博主君commit的时候手抖把页面关了，于是没截着图，下面这张图是在我得到最高分之后，用这次的结果重新make commission的，截了个图，得分是0.79426，不是目前我的最高分哈，因此排名木有变…)：</p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/025.png" alt=""></p>
<h3 id="9-3-learning-curves"><a href="#9-3-learning-curves" class="headerlink" title="9.3 learning curves"></a>9.3 learning curves</h3><p>有一个很可能发生的问题是，我们不断地做feature engineering，产生的特征越来越多，用这些特征去训练模型，会对我们的训练集拟合得越来越好，同时也可能在逐步丧失泛化能力，从而在待预测的数据上，表现不佳，也就是发生过拟合问题。</p>
<p>从另一个角度上说，如果模型在待预测的数据上表现不佳，除掉上面说的过拟合问题，也有可能是欠拟合问题，也就是说在训练集上，其实拟合的也不是那么好。</p>
<p>额，这个欠拟合和过拟合怎么解释呢。这么说吧：</p>
<ul>
<li>过拟合就像是你班那个学数学比较刻板的同学，老师讲过的题目，一字不漏全记下来了，于是老师再出一样的题目，分分钟精确出结果。but数学考试，因为总是碰到新题目，所以成绩不咋地。</li>
<li>欠拟合就像是，咳咳，和博主level差不多的差生。连老师讲的练习题也记不住，于是连老师出一样题目复习的周测都做不好，考试更是可想而知了。</li>
</ul>
<p>而在机器学习的问题上，对于过拟合和欠拟合两种情形。我们优化的方式是不同的。</p>
<p>对过拟合而言，通常以下策略对结果优化是有用的：</p>
<ul>
<li>做一下feature selection，挑出较好的feature的subset来做training</li>
<li>提供更多的数据，从而弥补原始数据的bias问题，学习到的model也会更准确</li>
</ul>
<p>而对于欠拟合而言，我们通常需要更多的feature，更复杂的模型来提高准确度。</p>
<p>著名的learning curve可以帮我们判定我们的模型现在所处的状态。我们以样本数为横坐标，训练和交叉验证集上的错误率作为纵坐标，两种状态分别如下两张图所示：过拟合(overfitting/high variace)，欠拟合(underfitting/high bias)</p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/026.png" alt=""></p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/027.png" alt=""></p>
<p>我们也可以把错误率替换成准确率(得分)，得到另一种形式的learning curve(sklearn 里面是这么做的)。</p>
<p>回到我们的问题，我们用scikit-learn里面的learning_curve来帮我们分辨我们模型的状态。举个例子，这里我们一起画一下我们最先得到的baseline model的learning curve。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.learning_curve <span class="keyword">import</span> learning_curve</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用sklearn的learning_curve得到training_score和cv_score，使用matplotlib画出learning curve</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span><span class="params">(estimator, title, X, y, ylim=None, cv=None, n_jobs=<span class="number">1</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                        train_sizes=np.linspace<span class="params">(<span class="number">.05</span>, <span class="number">1.</span>, <span class="number">20</span>)</span>, verbose=<span class="number">0</span>, plot=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    画出data在某模型上的learning curve.</span></span><br><span class="line"><span class="string">    参数解释</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    estimator : 你用的分类器。</span></span><br><span class="line"><span class="string">    title : 表格的标题。</span></span><br><span class="line"><span class="string">    X : 输入的feature，numpy类型</span></span><br><span class="line"><span class="string">    y : 输入的target vector</span></span><br><span class="line"><span class="string">    ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点</span></span><br><span class="line"><span class="string">    cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)</span></span><br><span class="line"><span class="string">    n_jobs : 并行的的任务数(默认1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, verbose=verbose)</span><br><span class="line"></span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> plot:</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.title(title)</span><br><span class="line">        <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            plt.ylim(*ylim)</span><br><span class="line">        plt.xlabel(<span class="string">u"训练样本数"</span>)</span><br><span class="line">        plt.ylabel(<span class="string">u"得分"</span>)</span><br><span class="line">        plt.gca().invert_yaxis()</span><br><span class="line">        plt.grid()</span><br><span class="line"></span><br><span class="line">        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, </span><br><span class="line">                         alpha=<span class="number">0.1</span>, color=<span class="string">"b"</span>)</span><br><span class="line">        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, </span><br><span class="line">                         alpha=<span class="number">0.1</span>, color=<span class="string">"r"</span>)</span><br><span class="line">        plt.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"b"</span>, label=<span class="string">u"训练集上得分"</span>)</span><br><span class="line">        plt.plot(train_sizes, test_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"r"</span>, label=<span class="string">u"交叉验证集上得分"</span>)</span><br><span class="line"></span><br><span class="line">        plt.legend(loc=<span class="string">"best"</span>)</span><br><span class="line"></span><br><span class="line">        plt.draw()</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.gca().invert_yaxis()</span><br><span class="line"></span><br><span class="line">    midpoint = ((train_scores_mean[<span class="number">-1</span>] + train_scores_std[<span class="number">-1</span>]) + (test_scores_mean[<span class="number">-1</span>] - test_scores_std[<span class="number">-1</span>])) / <span class="number">2</span></span><br><span class="line">    diff = (train_scores_mean[<span class="number">-1</span>] + train_scores_std[<span class="number">-1</span>]) - (test_scores_mean[<span class="number">-1</span>] - test_scores_std[<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">return</span> midpoint, diff</span><br><span class="line"></span><br><span class="line">plot_learning_curve(clf, <span class="string">u"学习曲线"</span>, X, y)</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/028.png" alt=""></p>
<p>在实际数据上看，我们得到的learning curve没有理论推导的那么光滑哈，但是可以大致看出来，训练集和交叉验证集上的得分曲线走势还是符合预期的。</p>
<p>目前的曲线看来，我们的model并不处于overfitting的状态(overfitting的表现一般是训练集上得分高，而交叉验证集上要低很多，中间的gap比较大)。因此我们可以再做些feature engineering的工作，添加一些新产出的特征或者组合特征到模型中。</p>
<h2 id="10-模型融合-model-ensemble"><a href="#10-模型融合-model-ensemble" class="headerlink" title="10.模型融合(model ensemble)"></a>10.模型融合(model ensemble)</h2><p>好了，终于到这一步了，我们要祭出机器学习/数据挖掘上通常最后会用到的大杀器了。恩，模型融合。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">『强迫症患者』打算继续喊喊口号… </span><br><span class="line">『模型融合(model ensemble)很重要！』 </span><br><span class="line">『模型融合(model ensemble)很重要！』 </span><br><span class="line">『模型融合(model ensemble)很重要！』</span><br></pre></td></tr></table></figure>
<p>重要的事情说三遍，恩，噢啦。</p>
<p>先解释解释，一会儿再回到我们的问题上哈。 </p>
<p>啥叫模型融合呢，我们还是举几个例子直观理解一下好了。</p>
<p>大家都看过知识问答的综艺节目中，求助现场观众时候，让观众投票，最高的答案作为自己的答案的形式吧，每个人都有一个判定结果，最后我们相信答案在大多数人手里。</p>
<p>再通俗一点举个例子。你和你班某数学大神关系好，每次作业都『模仿』他的，于是绝大多数情况下，他做对了，你也对了。突然某一天大神脑子犯糊涂，手一抖，写错了一个数，于是…恩，你也只能跟着错了。 </p>
<p>我们再来看看另外一个场景，你和你班5个数学大神关系都很好，每次都把他们作业拿过来，对比一下，再『自己做』，那你想想，如果哪天某大神犯糊涂了，写错了，but另外四个写对了啊，那你肯定相信另外4人的是正确答案吧？</p>
<p>最简单的模型融合大概就是这么个意思，比如分类问题，当我们手头上有一堆在同一份数据集上训练得到的分类器(比如logistic regression，SVM，KNN，random forest，神经网络)，那我们让他们都分别去做判定，然后对结果做投票统计，取票数最多的结果为最后结果。</p>
<p>bingo，问题就这么完美的解决了。</p>
<p>模型融合可以比较好地缓解，训练过程中产生的过拟合问题，从而对于结果的准确度提升有一定的帮助。</p>
<p>话说回来，回到我们现在的问题。你看，我们现在只讲了logistic regression，如果我们还想用这个融合思想去提高我们的结果，我们该怎么做呢？</p>
<p>既然这个时候模型没得选，那咱们就在数据上动动手脚咯。大家想想，如果模型出现过拟合现在，一定是在我们的训练上出现拟合过度造成的对吧。</p>
<p>那我们干脆就不要用全部的训练集，每次取训练集的一个subset，做训练，这样，我们虽然用的是同一个机器学习算法，但是得到的模型却是不一样的；同时，因为我们没有任何一份子数据集是全的，因此即使出现过拟合，也是在子训练集上出现过拟合，而不是全体数据上，这样做一个融合，可能对最后的结果有一定的帮助。对，这就是常用的Bagging。</p>
<p>我们用scikit-learn里面的Bagging来完成上面的思路，过程非常简单。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingRegressor</span><br><span class="line"></span><br><span class="line">train_df = df.filter(regex=<span class="string">'Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*|Mother|Child|Family|Title'</span>)</span><br><span class="line">train_np = train_df.as_matrix()</span><br><span class="line"></span><br><span class="line"><span class="comment"># y即Survival结果</span></span><br><span class="line">y = train_np[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># X即特征属性值</span></span><br><span class="line">X = train_np[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit到BaggingRegressor之中</span></span><br><span class="line">clf = linear_model.LogisticRegression(C=<span class="number">1.0</span>, penalty=<span class="string">'l1'</span>, tol=<span class="number">1e-6</span>)</span><br><span class="line">bagging_clf = BaggingRegressor(clf, n_estimators=<span class="number">20</span>, max_samples=<span class="number">0.8</span>, max_features=<span class="number">1.0</span>, bootstrap=<span class="keyword">True</span>, bootstrap_features=<span class="keyword">False</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">bagging_clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">test = df_test.filter(regex=<span class="string">'Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*|Mother|Child|Family|Title'</span>)</span><br><span class="line">predictions = bagging_clf.predict(test)</span><br><span class="line">result = pd.DataFrame(&#123;<span class="string">'PassengerId'</span>:data_test[<span class="string">'PassengerId'</span>].as_matrix(), <span class="string">'Survived'</span>:predictions.astype(np.int32)&#125;)</span><br><span class="line">result.to_csv(<span class="string">"/Users/HanXiaoyang/Titanic_data/logistic_regression_bagging_predictions.csv"</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>然后你再Make a submission，恩，发现对结果还是有帮助的。</p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/029.png" alt=""></p>
<h2 id="11-总结"><a href="#11-总结" class="headerlink" title="11.总结"></a>11.总结</h2><p>文章稍微有点长，非常感谢各位耐心看到这里。 </p>
<p>总结的部分，我就简短写几段，出现的话，很多在文中有对应的场景，大家有兴趣再回头看看。</p>
<p>对于任何的机器学习问题，不要一上来就追求尽善尽美，先用自己会的算法撸一个baseline的model出来，再进行后续的分析步骤，一步步提高。</p>
<p>在问题的结果过程中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">『对数据的认识太重要了！』</span><br><span class="line">『数据中的特殊点/离群点的分析和处理太重要了！』</span><br><span class="line">『特征工程(feature engineering)太重要了！』</span><br><span class="line">『模型融合(model ensemble)太重要了！』</span><br></pre></td></tr></table></figure>
<p>本文中用机器学习解决问题的过程大概如下图所示： </p>
<p><img src="/assets/images/2015/11/12/ml-logistic-regression-3/030.png" alt=""></p>
<h2 id="12-关于数据和代码"><a href="#12-关于数据和代码" class="headerlink" title="12.关于数据和代码"></a>12.关于数据和代码</h2><p>本文中的数据和代码已经上传至github中，欢迎大家下载和自己尝试。</p>
<hr>
<ul>
<li>原文链接：<a href="http://blog.csdn.net/han_xiaoyang/article/details/49797143" target="_blank" rel="noopener">机器学习系列(3)_逻辑回归应用之Kaggle泰坦尼克之灾</a></li>
</ul>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine-Learning</a>
          
            <a href="/tags/Logistic-Regression/" rel="tag"># Logistic-Regression</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/2015-11-05-succinct-on-apache-spark/" rel="next" title="Succinct on Apache Spark: Queries on Compressed RDDs">
                <i class="fa fa-chevron-left"></i> Succinct on Apache Spark: Queries on Compressed RDDs
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/2016-01-06-ml-overview/" rel="prev" title="机器学习系列(4)_机器学习算法一览，应用建议与解决思路">
                机器学习系列(4)_机器学习算法一览，应用建议与解决思路 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/assets/images/hyperj.jpg"
                alt="HyperJ" />
            
              <p class="site-author-name" itemprop="name">HyperJ</p>
              <p class="site-description motion-element" itemprop="description">Data, Distribution, Containerization, Artificial Intelligence</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">180</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">63</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">197</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://gist.github.com/HyperJ/" target="_blank" title="Gist" rel="external nofollow"><i class="fa fa-fw fa-github-alt"></i>Gist</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://github.com/HyperJ" target="_blank" title="GitHub" rel="external nofollow"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://blog.hyperj.net" target="_blank" title="Blog" rel="external nofollow"><i class="fa fa-fw fa-bookmark-o"></i>Blog</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.linkedin.com/in/hyperj" target="_blank" title="Linkedin" rel="external nofollow"><i class="fa fa-fw fa-linkedin"></i>Linkedin</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://blog.csdn.net/han_xiaoyang" title="寒小阳" target="_blank">寒小阳</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://blog.csdn.net/odailidong" title="dailidong" target="_blank">dailidong</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://dmlcoding.com" title="Time渐行渐远" target="_blank">Time渐行渐远</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://91fz.org" title="程序员疯子" target="_blank">程序员疯子</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.bcmeng.com" title="编程小梦" target="_blank">编程小梦</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://hexiaoqiao.github.io" title="Hexiaoqiao" target="_blank">Hexiaoqiao</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.jianshu.com/u/90ab66c248e6" title="占小狼" target="_blank">占小狼</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-引言"><span class="nav-text">1.引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-背景"><span class="nav-text">2.背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-关于Kaggle"><span class="nav-text">2.1 关于Kaggle</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-关于泰坦尼克号之灾"><span class="nav-text">2.2 关于泰坦尼克号之灾</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-说明"><span class="nav-text">3.说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-怎么做？"><span class="nav-text">4.怎么做？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-初探数据"><span class="nav-text">5.初探数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-数据初步分析"><span class="nav-text">6.数据初步分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-乘客各属性分布"><span class="nav-text">6.1 乘客各属性分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-属性与获救结果的关联统计"><span class="nav-text">6.2 属性与获救结果的关联统计</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-简单数据预处理"><span class="nav-text">7.简单数据预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-逻辑回归建模"><span class="nav-text">8.逻辑回归建模</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-逻辑回归系统优化"><span class="nav-text">9.逻辑回归系统优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-模型系数关联分析"><span class="nav-text">9.1 模型系数关联分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-交叉验证"><span class="nav-text">9.2 交叉验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-learning-curves"><span class="nav-text">9.3 learning curves</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-模型融合-model-ensemble"><span class="nav-text">10.模型融合(model ensemble)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-总结"><span class="nav-text">11.总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-关于数据和代码"><span class="nav-text">12.关于数据和代码</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2009 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-[object Object]"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HyperJ</span>

  

  
</div>


  



  <div class="powered-by">Powered by <a class="theme-link" target="_blank" rel="external nofollow" href="https://hexo.io">Hexo</a></div>








        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="Total Visitors">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="Total Views">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.2.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.2.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.2.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.2.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.2.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.2.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.2.0"></script>



  



	





  





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.json";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  

  

  

  
  <script type="text/javascript" src="/js/src/exturl.js?v=6.2.0"></script>


  

</body>
</html>
